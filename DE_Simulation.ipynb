{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63adb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) Setup & artifacts  \n",
    "# =========================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEFAULT_DTYPE = torch.float32\n",
    "torch.set_default_dtype(DEFAULT_DTYPE)\n",
    "\n",
    "#model architecture\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_neurons, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_neurons)\n",
    "        self.fc2 = nn.Linear(hidden_neurons, hidden_neurons)\n",
    "        self.fc3 = nn.Linear(hidden_neurons, hidden_neurons)\n",
    "        self.fc7 = nn.Linear(hidden_neurons, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = F.leaky_relu(self.fc1(x)); hidden = self.dropout(hidden)\n",
    "        hidden = F.leaky_relu(self.fc2(hidden)); hidden = self.dropout(hidden)\n",
    "        hidden = F.leaky_relu(self.fc3(hidden)); hidden = self.dropout(hidden)\n",
    "        out = self.fc7(hidden)\n",
    "        return out\n",
    "\n",
    "ARTIFACT_DIR   = \"artifacts\"\n",
    "MODEL_PATH     = os.path.join(ARTIFACT_DIR, \"mlp_boiler_state_dict.pth\")\n",
    "CONFIG_PATH    = os.path.join(ARTIFACT_DIR, \"model_config.json\")\n",
    "X_SCALER_PATH  = os.path.join(ARTIFACT_DIR, \"X_scaler_global.joblib\")  \n",
    "Y_SCALER_PATH  = os.path.join(ARTIFACT_DIR, \"Y_scaler_global.joblib\")  \n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "input_width        = int(config[\"input_width\"])\n",
    "output_width       = int(config[\"output_width\"])\n",
    "num_input_features = int(config[\"num_input_features\"])\n",
    "hidden_units       = int(config[\"hidden_neurons\"])\n",
    "output_dim         = int(config[\"output_dim\"])  \n",
    "input_dim          = input_width * num_input_features \n",
    "\n",
    "#instantiate model and load weights\n",
    "model = MLP(input_dim=input_dim, output_dim=output_dim, hidden_neurons=hidden_units, dropout_rate=0.2).to(device)\n",
    "state_dict = torch.load(MODEL_PATH, map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "X_scaler = joblib.load(X_SCALER_PATH) \n",
    "Y_scaler = joblib.load(Y_SCALER_PATH)  \n",
    "\n",
    "\n",
    "# B) Data shaping & scaling helpers\n",
    "# =========================\n",
    "\n",
    "#column names\n",
    "input_names = [\n",
    "    \"Coal1\",\"Coal2\",\"Coal3\",\"Coal4\",\"Coal5\",\n",
    "    \"PA_Flow\",\"SA_Flow\",\n",
    "    \"OFA_p1\",\"OFA_p2\",\"OFA_p3\",\"OFA_p4\",\n",
    "    \"T_amb\"\n",
    "]\n",
    "target_col_Q = \"HeatLoad\"\n",
    "\n",
    "def matrix_to_flat(X: np.ndarray) -> np.ndarray:\n",
    "    X = np.asarray(X)\n",
    "    return X.reshape(-1)\n",
    "\n",
    "#MinMaxScaler parameters\n",
    "_mm_scale  = X_scaler.scale_\n",
    "_mm_minoff = X_scaler.min_\n",
    "\n",
    "def phys_to_scaled_feature(i: int, x_phys: np.ndarray) -> np.ndarray:\n",
    "    return x_phys * _mm_scale[i] + _mm_minoff[i]\n",
    "\n",
    "def scaled_to_phys_feature(i: int, x_scaled: np.ndarray) -> np.ndarray:\n",
    "    return (x_scaled - _mm_minoff[i]) / _mm_scale[i]\n",
    "\n",
    "def robust_scale_Q_phys(Q_phys: np.ndarray) -> np.ndarray:\n",
    "    center = Y_scaler.center_[0]\n",
    "    scale  = Y_scaler.scale_[0]\n",
    "    return (Q_phys - center) / scale\n",
    "\n",
    "\n",
    "#load dummy inputs & desired heat load (Q_star)\n",
    "SIM_CSV = \"Sim_Test5_MULTIPLE.csv\"\n",
    "df_sim = pd.read_csv(SIM_CSV)\n",
    "\n",
    "#extract physical arrays\n",
    "X_phys_full  = df_sim[input_names].to_numpy(dtype=float)           \n",
    "Q_star_phys  = df_sim[target_col_Q].to_numpy(dtype=float)         \n",
    "N_STEPS, N_FEATURES_FULL = X_phys_full.shape\n",
    "\n",
    "# Indices\n",
    "name_to_idx = {name: i for i, name in enumerate(input_names)}\n",
    "i_Tamb = name_to_idx[\"T_amb\"]\n",
    "decision_feature_indices = [i for i in range(N_FEATURES_FULL) if i != i_Tamb]  # remove T_amb\n",
    "N_DECISION_FEATURES = len(decision_feature_indices)  # 11\n",
    "\n",
    "#fixed ambient (scaled)\n",
    "Tamb_fixed_phys   = 295.0\n",
    "Tamb_fixed_scaled = float(phys_to_scaled_feature(i_Tamb, np.array([Tamb_fixed_phys]))[0])\n",
    "\n",
    "#scale decision features with MinMax per-feature\n",
    "X_scaled_decision = np.empty((N_STEPS, N_DECISION_FEATURES), dtype=float)\n",
    "for k, j in enumerate(decision_feature_indices):\n",
    "    X_scaled_decision[:, k] = phys_to_scaled_feature(j, X_phys_full[:, j])\n",
    "\n",
    "#targets in scaled Y-space\n",
    "yQ_target_scaled   = robust_scale_Q_phys(Q_star_phys)                              \n",
    "\n",
    "def reconstruct_full_scaled_single(x_dec_scaled_row: np.ndarray) -> np.ndarray:\n",
    "    X_full = np.empty((N_FEATURES_FULL,), dtype=float)\n",
    "    for k, j in enumerate(decision_feature_indices):\n",
    "        X_full[j] = x_dec_scaled_row[k]\n",
    "    X_full[i_Tamb] = Tamb_fixed_scaled\n",
    "    return X_full\n",
    "\n",
    "def hard_project_coal_decision_scaled_row(x_dec_scaled_row: np.ndarray) -> np.ndarray:\n",
    "    #row-wise strict coal feasibility in physical units, mapped back to scaled.\n",
    "\n",
    "    xr = np.array(x_dec_scaled_row, dtype=float, copy=True)\n",
    "    for kd, j_full in zip(coal_indices_decision, coal_indices_full):\n",
    "        x_phys = scaled_to_phys_feature(j_full, xr[kd])\n",
    "        x_phys_proj = 0.0 if (x_phys < coal_threshold_phys) else float(np.clip(x_phys, coal_threshold_phys, coal_hi_phys))\n",
    "        xr[kd] = phys_to_scaled_feature(j_full, np.array([x_phys_proj]))[0]\n",
    "    return xr\n",
    "\n",
    "#coal feature indices in the full 12-col space\n",
    "coal_indices_full   = [name_to_idx[f\"Coal{i}\"] for i in range(1, 6)]\n",
    "coal_threshold_phys = 14.5\n",
    "coal_hi_phys        = 34.0\n",
    "\n",
    "coal_indices_decision = [decision_feature_indices.index(j) for j in coal_indices_full]\n",
    "\n",
    "\n",
    "# C & D) Objective\n",
    "# =========================\n",
    "\n",
    "w_Q   = 1\n",
    "w_eff = 0.000014\n",
    "\n",
    "PRINT_EVERY = 5 \n",
    "\n",
    "#DE knobs\n",
    "strategy = \"best1bin\"\n",
    "popsize  = 10\n",
    "maxiter  = 25\n",
    "mutation = 0.4\n",
    "recomb   = 0.6\n",
    "\n",
    "def objective_single_step(x_dec_scaled_row: np.ndarray, yQ_target_scaled_t: float) -> float:\n",
    "    \n",
    "    #Minimize: J = w_Q * (yQ_pred - yQ_target)^2  -  w_eff * yEff_pred\n",
    "\n",
    "    x_dec_feas = hard_project_coal_decision_scaled_row(x_dec_scaled_row)\n",
    "\n",
    "    x_full_scaled = reconstruct_full_scaled_single(x_dec_feas)\n",
    "\n",
    "    #model forward\n",
    "    with torch.no_grad():\n",
    "        y_scaled = model(torch.from_numpy(x_full_scaled[None, :]).to(device=device, dtype=DEFAULT_DTYPE)).cpu().numpy()[0]\n",
    "\n",
    "\n",
    "    yQ_pred_scaled   = float(y_scaled[0])\n",
    "    yEff_pred_scaled = float(y_scaled[1])\n",
    "\n",
    "    cost_Q = (yQ_pred_scaled - yQ_target_scaled_t) ** 2\n",
    "    J = w_Q * cost_Q - w_eff * yEff_pred_scaled\n",
    "    return float(J)\n",
    "\n",
    "\n",
    "\n",
    "# E) Bounds (scaled)\n",
    "# =========================\n",
    "\n",
    "# Build scaled bounds per decision feature from PHYSICAL limits\n",
    "physical_bounds = {\n",
    "    \"Coal1\": (0.0, 34.0),\n",
    "    \"Coal2\": (0.0, 34.0),\n",
    "    \"Coal3\": (0.0, 34.0),\n",
    "    \"Coal4\": (0.0, 34.0),\n",
    "    \"Coal5\": (0.0, 34.0),\n",
    "    \"PA_Flow\": (40.0, 200.0),\n",
    "    \"SA_Flow\": (300.0, 560.0),\n",
    "    \"OFA_p1\": (8.0, 35.0),\n",
    "    \"OFA_p2\": (8.0, 35.0),\n",
    "    \"OFA_p3\": (8.0, 35.0),\n",
    "    \"OFA_p4\": (8.0, 35.0),\n",
    "    \"T_amb\": (295.0, 295.0),   \n",
    "}\n",
    "\n",
    "bounds_decision_single = []\n",
    "for j in decision_feature_indices:\n",
    "    lo_phys, hi_phys = physical_bounds[input_names[j]]\n",
    "    lo_scaled = float(phys_to_scaled_feature(j, np.array([lo_phys]))[0])\n",
    "    hi_scaled = float(phys_to_scaled_feature(j, np.array([hi_phys]))[0])\n",
    "    lo_s, hi_s = (min(lo_scaled, hi_scaled), max(lo_scaled, hi_scaled))\n",
    "    bounds_decision_single.append((lo_s, hi_s))\n",
    "\n",
    "lb_single = np.array([b[0] for b in bounds_decision_single], dtype=float)\n",
    "ub_single = np.array([b[1] for b in bounds_decision_single], dtype=float)\n",
    "\n",
    "\n",
    "# F) Optimiser run — Differential Evolution (segment-wise, minimal)\n",
    "# =========================\n",
    "\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "# --- Segment detection on PHYSICAL HeatLoad targets ---\n",
    "def find_constant_segments(Q_phys: np.ndarray, rel_tol=2e-3, abs_tol=0.05):\n",
    "    Q = np.asarray(Q_phys, dtype=float)\n",
    "    if Q.size <= 1:\n",
    "        return [np.arange(Q.size)]\n",
    "    dQ = np.abs(np.diff(Q))\n",
    "    thresh = np.maximum(abs_tol, rel_tol * np.maximum(1.0, np.abs(Q[:-1])))\n",
    "    edges = np.flatnonzero(dQ > thresh) + 1\n",
    "    return np.split(np.arange(Q.size), edges)\n",
    "\n",
    "segments = find_constant_segments(Q_star_phys, rel_tol=2e-3, abs_tol=0.05)\n",
    "print(f\"[INFO] Found {len(segments)} constant segments in HeatLoad target.\")\n",
    "\n",
    "\n",
    "def make_init_population(seed_row: np.ndarray, ndim: int, seed: int) -> np.ndarray:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n_pop = popsize * ndim\n",
    "    init_pop = np.empty((n_pop, ndim), dtype=float)\n",
    "    init_pop[0] = np.clip(seed_row, lb_single, ub_single)\n",
    "    for idx in range(1, n_pop):\n",
    "        r = rng.random(ndim)\n",
    "        init_pop[idx] = lb_single + r * (ub_single - lb_single)\n",
    "    return init_pop\n",
    "\n",
    "def optimize_single_step(t_idx: int, seed_row: np.ndarray, seed: int) -> np.ndarray:\n",
    "    yQ_t = float(yQ_target_scaled[t_idx])\n",
    "    ndim = N_DECISION_FEATURES\n",
    "    init_pop = make_init_population(seed_row, ndim, seed)\n",
    "\n",
    "\n",
    "    progress = {\"gen\": 0, \"bestJ\": np.inf}\n",
    "    def _callback(best_x, convergence):\n",
    "        progress[\"gen\"] += 1\n",
    "        J = objective_single_step(best_x, yQ_t)\n",
    "        dJ = (progress[\"bestJ\"] - J) if np.isfinite(progress[\"bestJ\"]) else np.nan\n",
    "        if J < progress[\"bestJ\"]:\n",
    "            progress[\"bestJ\"] = J\n",
    "        if (progress[\"gen\"] % PRINT_EVERY == 0) or (progress[\"gen\"] == 1):\n",
    "            print(f\"[DE:t={t_idx:4d}] gen {progress['gen']:3d} | J_best={J:.6e} | ΔJ={(0 if np.isnan(dJ) else dJ):.3e} | conv={convergence:.3e}\")\n",
    "        return False  \n",
    "\n",
    "    result = differential_evolution(\n",
    "        func=lambda x: objective_single_step(x, yQ_t),\n",
    "        bounds=bounds_decision_single,\n",
    "        strategy=strategy,\n",
    "        maxiter=maxiter,\n",
    "        popsize=popsize,\n",
    "        mutation=mutation,\n",
    "        recombination=recomb,\n",
    "        seed=seed,\n",
    "        updating=\"deferred\",   \n",
    "        workers=1,           \n",
    "        polish=True,\n",
    "        init=init_pop,\n",
    "        callback=_callback,\n",
    "    )\n",
    "\n",
    "    print(f\"[DE:t={t_idx:4d}] done | nit={result.nit:3d} | nfev={result.nfev:5d} | J*={result.fun:.6e} | status: {result.message}\")\n",
    "\n",
    "    # Return projected optimum\n",
    "    return hard_project_coal_decision_scaled_row(result.x)\n",
    "\n",
    "X_best_decision_scaled = np.empty((N_STEPS, N_DECISION_FEATURES), dtype=float)\n",
    "\n",
    "for segment_idx, segment_indices in enumerate(segments):\n",
    "    #independent seed & independent seed row per segment\n",
    "    seg_seed = 12345 + segment_idx\n",
    "    seed_segment_row = np.clip(np.mean(X_scaled_decision[segment_indices, :], axis=0), lb_single, ub_single)\n",
    "    mid_timestep = int(segment_indices[len(segment_indices)//2])\n",
    "    optimal_row = optimize_single_step(t_idx=mid_timestep, seed_row=seed_segment_row, seed=seg_seed)\n",
    "    X_best_decision_scaled[segment_indices, :] = optimal_row\n",
    "\n",
    "for t in range(N_STEPS):\n",
    "    X_best_decision_scaled[t, :] = hard_project_coal_decision_scaled_row(X_best_decision_scaled[t, :])\n",
    "\n",
    "n_unique = np.unique(X_best_decision_scaled, axis=0).shape[0]\n",
    "print(f\"[DEBUG] Unique decision rows: {n_unique} (segments: {len(segments)})\")\n",
    "\n",
    "x_best_scaled_flat = matrix_to_flat(X_best_decision_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7992db51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G) Export optimised schedule + predictions\n",
    "# =========================\n",
    "if \"X_best_decision_scaled\" not in locals():\n",
    "    raise RuntimeError(\"no optimised decision schedule found. did section f run?\")\n",
    "\n",
    "#insert fixed t_amb and build full 12-col scaled inputs\n",
    "X_best_full_scaled = np.empty((N_STEPS, N_FEATURES_FULL), dtype=float)\n",
    "for k, j in enumerate(decision_feature_indices):\n",
    "    X_best_full_scaled[:, j] = X_best_decision_scaled[:, k]\n",
    "X_best_full_scaled[:, i_Tamb] = Tamb_fixed_scaled\n",
    "\n",
    "#invert inputs to physical units\n",
    "X_best_phys = X_scaler.inverse_transform(X_best_full_scaled).astype(np.float64)\n",
    "\n",
    "#model predictions at the best schedule (scaled → physical)\n",
    "with torch.no_grad():\n",
    "    y_scaled_best = model(\n",
    "        torch.from_numpy(X_best_full_scaled).to(device=device, dtype=DEFAULT_DTYPE)\n",
    "    ).cpu().numpy()\n",
    "\n",
    "Y_best_phys    = Y_scaler.inverse_transform(y_scaled_best).astype(np.float64)\n",
    "yQ_best_phys   = Y_best_phys[:, 0]\n",
    "yEff_best_phys = Y_best_phys[:, 1]\n",
    "\n",
    "#build output dataframe (numeric) + predictions\n",
    "df_out = pd.DataFrame(X_best_phys, columns=input_names)\n",
    "df_out[\"HeatLoad\"]   = yQ_best_phys\n",
    "df_out[\"Efficiency\"] = yEff_best_phys\n",
    "\n",
    "#add sumcoal = coal1+...+coal5\n",
    "coal_cols = [\"Coal1\", \"Coal2\", \"Coal3\", \"Coal4\", \"Coal5\"]\n",
    "df_out[\"SumCoal\"] = df_out[coal_cols].sum(axis=1).astype(np.float64)\n",
    "\n",
    "#save to csv with fixed-point formatting (no scientific notation)\n",
    "OUT_CSV = \"Sim_Test5_optimised.csv\"\n",
    "df_out.to_csv(OUT_CSV, index=False, float_format=\"%.6f\")\n",
    "\n",
    "\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)\n",
    "print(df_out.iloc[7, 14])\n",
    "#print(df_out.iloc[14, 14])\n",
    "#print(df_out.iloc[15, 14])\n",
    "print(df_out.iloc[28, 14])\n",
    "#print(df_out.iloc[40, 14])\n",
    "print(df_out.iloc[56, 14])\n",
    "print(df_out.iloc[56, 13])\n",
    "#print(df_out.iloc[71, 14])\n",
    "print(df_out.iloc[78, 14])\n",
    "#print(df_out.iloc[84, 14])\n",
    "print(df_out.iloc[92, 14])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
