{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27832a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "from itertools import cycle\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "#establish GPU device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#configuration\n",
    "input_width = 1\n",
    "output_width = 1\n",
    "offset = 1\n",
    "batch_size = 128 \n",
    "epochs = 1000\n",
    "\n",
    "patience = 180  #number of epochs with no improvement after which training will be stopped\n",
    "min_delta = 1e-5  #minimum change in the monitored quantity to qualify as an improvement\n",
    "\n",
    "#load datasets\n",
    "csv_paths = [f\"Unit2_dataset_rev1_{i}.csv\" for i in range(1, 8)] + \\\n",
    "            [f\"Unit4_dataset_rev1_{i}.csv\" for i in range(1, 9)]\n",
    "\n",
    "datasets_raw = []  #list of (X_raw, Y_raw) tuples\n",
    "\n",
    "for path in csv_paths:\n",
    "    df = pd.read_csv(path)\n",
    "    X_raw = df[[f\"X{i}\" for i in range(1, 13)]].values  #12 inputs\n",
    "    Y_raw = df[[\"Y1\", \"Y2\"]].values  #Y1 = heat load; Y2 = efficiency\n",
    "    datasets_raw.append((X_raw, Y_raw))\n",
    "\n",
    "#split data (70/20/10)\n",
    "datasets_split = []  #list of (X_train, X_val, X_test, Y_train, Y_val, Y_test)\n",
    "\n",
    "for X_raw, Y_raw in datasets_raw:\n",
    "    n = len(X_raw)\n",
    "    train_end = int(n * 0.7)\n",
    "    val_end = int(n * 0.9)\n",
    "\n",
    "    X_train = X_raw[:train_end]\n",
    "    X_val   = X_raw[train_end:val_end]\n",
    "    X_test  = X_raw[val_end:]\n",
    "\n",
    "    Y_train = Y_raw[:train_end]\n",
    "    Y_val   = Y_raw[train_end:val_end]\n",
    "    Y_test  = Y_raw[val_end:]\n",
    "\n",
    "    datasets_split.append((X_train, X_val, X_test, Y_train, Y_val, Y_test))\n",
    "\n",
    "#global scaler on all train data\n",
    "X_train_all = np.concatenate([X_train for (X_train, X_val, X_test, Y_train, Y_val, Y_test) in datasets_split], axis=0)\n",
    "Y_train_all = np.concatenate([Y_train for (X_train, X_val, X_test, Y_train, Y_val, Y_test) in datasets_split], axis=0)\n",
    "\n",
    "X_scaler_global = MinMaxScaler()\n",
    "Y_scaler_global = RobustScaler()\n",
    "\n",
    "X_scaler_global.fit(X_train_all)\n",
    "Y_scaler_global.fit(Y_train_all)\n",
    "\n",
    "\n",
    "#scaling (fit only on train, transform train/val/test)\n",
    "datasets_scaled = []  #list of (X_train_scaled, X_val_scaled, X_test_scaled, Y_train_scaled, Y_val_scaled, Y_test_scaled)\n",
    "\n",
    "\n",
    "# >>> CHANGED <<<  # Use global scalers; no per-dataset fit\n",
    "datasets_scaled = []  # (X_train_scaled, X_val_scaled, X_test_scaled, Y_train_scaled, Y_val_scaled, Y_test_scaled)\n",
    "\n",
    "for X_train, X_val, X_test, Y_train, Y_val, Y_test in datasets_split:\n",
    "    X_train_scaled = X_scaler_global.transform(X_train)\n",
    "    X_val_scaled   = X_scaler_global.transform(X_val)\n",
    "    X_test_scaled  = X_scaler_global.transform(X_test)\n",
    "\n",
    "    Y_train_scaled = Y_scaler_global.transform(Y_train)\n",
    "    Y_val_scaled   = Y_scaler_global.transform(Y_val)\n",
    "    Y_test_scaled  = Y_scaler_global.transform(Y_test)\n",
    "\n",
    "    datasets_scaled.append((X_train_scaled, X_val_scaled, X_test_scaled, Y_train_scaled, Y_val_scaled, Y_test_scaled))\n",
    "\n",
    "    \n",
    "#sliding window function\n",
    "def make_windows(x_scaled, y_scaled):\n",
    "    #initialize lists for input and target windows\n",
    "    x_windows = []\n",
    "    y_windows = []\n",
    "\n",
    "    #maximum valid start index for a full window\n",
    "    max_start_idx = len(x_scaled) - input_width - offset - output_width + 1\n",
    "\n",
    "    #generate overlapping windows\n",
    "    for start_idx in range(max_start_idx):\n",
    "        x_window = x_scaled[start_idx : start_idx + input_width, :].flatten()\n",
    "\n",
    "        y_start = start_idx + input_width - 1 + offset\n",
    "        y_end = y_start + output_width\n",
    "        y_window = y_scaled[y_start:y_end].flatten()\n",
    "\n",
    "        x_windows.append(x_window)\n",
    "        y_windows.append(y_window)\n",
    "\n",
    "    #convert lists to numpy arrays\n",
    "    X_windows = np.array(x_windows, dtype=np.float32)\n",
    "    Y_windows = np.array(y_windows, dtype=np.float32)\n",
    "    return X_windows, Y_windows\n",
    "\n",
    "\n",
    "#apply sliding window to each set of data\n",
    "datasets_windowed = []  #list of (X_train_win, X_val_win, X_test_win, Y_train_win, Y_val_win, Y_test_win)\n",
    "\n",
    "for X_train, X_val, X_test, Y_train, Y_val, Y_test in datasets_scaled:\n",
    "    X_train_win, Y_train_win = make_windows(X_train, Y_train)\n",
    "    X_val_win,   Y_val_win   = make_windows(X_val,   Y_val)\n",
    "    X_test_win,  Y_test_win  = make_windows(X_test,  Y_test)\n",
    "\n",
    "    datasets_windowed.append((X_train_win, X_val_win, X_test_win,Y_train_win, Y_val_win, Y_test_win))\n",
    "\n",
    "#torch tensors and dataloaders; tensor = multidimensional array optimised for GPU; dataloader = makes batches\n",
    "train_loaders = []\n",
    "val_loaders = []\n",
    "test_loaders = []\n",
    "\n",
    "for X_train, X_val, X_test, Y_train, Y_val, Y_test in datasets_windowed:\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(Y_train))\n",
    "    val_dataset   = TensorDataset(torch.tensor(X_val),   torch.tensor(Y_val))\n",
    "    test_dataset  = TensorDataset(torch.tensor(X_test),  torch.tensor(Y_test))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_loaders.append(train_loader)\n",
    "    val_loaders.append(val_loader)\n",
    "    test_loaders.append(test_loader)\n",
    "\n",
    "#MLP class definition\n",
    "#dropout for regularisation\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, din, dout, num_neurons, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(din, num_neurons)\n",
    "        self.fc2 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.fc3 = nn.Linear(num_neurons, num_neurons)\n",
    "       # self.fc4 = nn.Linear(num_neurons, num_neurons)\n",
    "       # self.fc5 = nn.Linear(num_neurons, num_neurons)\n",
    "       # self.fc6 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.fc7 = nn.Linear(num_neurons, dout)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = nn.functional.leaky_relu(self.fc1(x))\n",
    "        h = self.dropout(h)\n",
    "        h = nn.functional.leaky_relu(self.fc2(h))\n",
    "        h = self.dropout(h)\n",
    "        h = nn.functional.leaky_relu(self.fc3(h))\n",
    "        h = self.dropout(h)\n",
    "     #   h = nn.functional.leaky_relu(self.fc4(h))\n",
    "      #  h = self.dropout(h)\n",
    "       # h = nn.functional.leaky_relu(self.fc5(h))\n",
    "      #  h = self.dropout(h)\n",
    "       # h = nn.functional.leaky_relu(self.fc6(h))\n",
    "       # h = self.dropout(h)\n",
    "        h = self.fc7(h)\n",
    "        return h\n",
    "    \n",
    "#model initialisation\n",
    "num_input_features = 12  #X1 to X12\n",
    "input_dim = input_width * num_input_features  \n",
    "output_dim = output_width * 2  #2 outputs\n",
    "hidden_neurons = 64\n",
    "\n",
    "model = MLP(input_dim, output_dim, hidden_neurons, dropout_rate=0.2).to(device)\n",
    "\n",
    "#Adam uses momentum - better than SGD\n",
    "#weight decay helps prevent overfitting by penalising large weights\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.5e-4)\n",
    "\n",
    "#times LR by factor if no improvement after patience \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=20, min_lr=1e-6\n",
    ")\n",
    "\n",
    "#appropriate for outliers - precision of MSE for small errors and robustness of MAE for outliers\n",
    "#SmoothL1(x) = \n",
    "#     0.5 * x²     if |x| < 1\n",
    "#    |x| - 0.5     if |x| ≥ 1\n",
    "criterion = torch.nn.SmoothL1Loss()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "best_model_state = None\n",
    "\n",
    "\n",
    "from itertools import zip_longest\n",
    "\n",
    "#interleaved A1 B1 C1 -> A2 B2 C2\n",
    "def round_robin_batches(loaders):\n",
    "    #convert each DataLoader to a list of batches\n",
    "    loaders_batches = [list(loader) for loader in loaders]\n",
    "\n",
    "    #zip_longest interleaves them round-robin style\n",
    "    for batch_group in zip_longest(*loaders_batches):\n",
    "        for batch in batch_group:\n",
    "            if batch is not None:\n",
    "                yield batch\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_epoch = None\n",
    "lr_history = []\n",
    "\n",
    "\n",
    "#train loop\n",
    "for epoch in range(epochs):\n",
    "    total_train_loss = 0.0\n",
    "    total_train_samples = 0\n",
    "    model.train()\n",
    "\n",
    "    round_robin_iter = round_robin_batches(train_loaders)\n",
    "\n",
    "    for batch_X, batch_Y in round_robin_iter:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_Y = batch_Y.to(device)\n",
    "\n",
    "        predictions = model(batch_X)\n",
    "        loss = criterion(predictions, batch_Y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_train_loss += loss.item() * batch_X.size(0)\n",
    "        total_train_samples += batch_X.size(0)\n",
    "\n",
    "    avg_train_loss = total_train_loss / total_train_samples\n",
    "    \n",
    "\n",
    "    #validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    total_val_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_loader in val_loaders:\n",
    "            for batch_X, batch_Y in val_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_Y = batch_Y.to(device)\n",
    "\n",
    "                val_preds = model(batch_X)\n",
    "                val_loss = criterion(val_preds, batch_Y)\n",
    "\n",
    "                total_val_loss += val_loss.item() * batch_X.size(0)\n",
    "                total_val_samples += batch_X.size(0)\n",
    "\n",
    "    avg_val_loss = total_val_loss / total_val_samples\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    lr_history.append(current_lr)   \n",
    "\n",
    "    \n",
    "    #early stopping\n",
    "    if avg_val_loss + min_delta < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        best_model_state = model.state_dict()\n",
    "        best_epoch = epoch\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epoch % 50 == 0 or epoch == epochs - 1:\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.6f} - Val Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "#restore best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Directory to save artifacts\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "\n",
    "# Save model state_dict\n",
    "MODEL_PATH = \"artifacts/mlp_boiler_state_dict.pth\"\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "\n",
    "# Save config needed for inference\n",
    "config = {\n",
    "    \"input_width\": int(input_width),\n",
    "    \"output_width\": int(output_width),\n",
    "    \"num_input_features\": int(num_input_features),\n",
    "    \"hidden_neurons\": int(hidden_neurons),\n",
    "    \"output_dim\": int(output_dim),\n",
    "    \"offset\": int(offset)\n",
    "}\n",
    "CONFIG_PATH = \"artifacts/model_config.json\"\n",
    "with open(CONFIG_PATH, \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"Saved model to {MODEL_PATH}\")\n",
    "print(f\"Saved config to {CONFIG_PATH}\")\n",
    "\n",
    "# >>> NEW <<<  # Save global scalers for inference\n",
    "joblib.dump(X_scaler_global, \"artifacts/X_scaler_global.joblib\")\n",
    "joblib.dump(Y_scaler_global, \"artifacts/Y_scaler_global.joblib\")\n",
    "print(\"Saved scalers to artifacts/X_scaler_global.joblib and artifacts/Y_scaler_global.joblib\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "epochs_ran = len(train_losses)\n",
    "epoch_axis = list(range(1, epochs_ran + 1))\n",
    "\n",
    "#plot training and validation losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epoch_axis, train_losses, label='train loss')\n",
    "plt.plot(epoch_axis, val_losses, label='validation loss')\n",
    "\n",
    "#mark the best epoch with a star if available\n",
    "if best_epoch is not None and 0 <= best_epoch < epochs_ran:\n",
    "    plt.scatter(\n",
    "        best_epoch + 1,\n",
    "        val_losses[best_epoch],\n",
    "        marker='*',\n",
    "        s=200,\n",
    "        label='best (early-stop restore)',\n",
    "    )\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('training and validation loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#print best epoch info\n",
    "if best_epoch is not None:\n",
    "    print(f\"best epoch (model restored): {best_epoch + 1}  |  val loss: {val_losses[best_epoch]:.6f}\")\n",
    "\n",
    "#plot learning rate over epochs\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(epoch_axis, lr_history, label='learning rate')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('learning rate')\n",
    "plt.title('learning rate over epochs')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#print final learning rate value\n",
    "print(f\"final learning rate: {lr_history[-1]:.6e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729780fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "#load the single global scaler saved after training if wanted\n",
    "#Y_scaler_global = joblib.load(\"artifacts/Y_scaler_global.joblib\")\n",
    "\n",
    "#select dataset\n",
    "dataset_idx = 6  \n",
    "\n",
    "x_test_win_scaled = datasets_windowed[dataset_idx][2]\n",
    "y_test_win_scaled = datasets_windowed[dataset_idx][5]\n",
    "\n",
    "#model inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_test_tensor = torch.tensor(x_test_win_scaled, dtype=torch.float32).to(device)\n",
    "    preds_scaled = model(x_test_tensor).cpu().numpy()\n",
    "\n",
    "#invert scaling to original units\n",
    "preds_unscaled = Y_scaler_global.inverse_transform(preds_scaled)\n",
    "targets_unscaled = Y_scaler_global.inverse_transform(y_test_win_scaled)\n",
    "\n",
    "#split interleaved targets: even columns → y1 (heat load), odd columns → y2 (efficiency)\n",
    "pred_y1 = preds_unscaled[:, ::2].flatten()\n",
    "pred_y2 = preds_unscaled[:, 1::2].flatten()\n",
    "true_y1 = targets_unscaled[:, ::2].flatten()\n",
    "true_y2 = targets_unscaled[:, 1::2].flatten()\n",
    "\n",
    "#heat load (y1)\n",
    "true_y1_dim = true_y1 / 1e8\n",
    "pred_y1_dim = pred_y1 / 1e8\n",
    "\n",
    "#time axis in days (96 samples = 1 day)\n",
    "x_days_y1 = np.arange(len(true_y1_dim)) / 96.0\n",
    "\n",
    "plt.figure(figsize=(11, 5))\n",
    "plt.plot(x_days_y1, true_y1_dim, label='actual heat load', linewidth=1.5)\n",
    "plt.plot(x_days_y1, pred_y1_dim, label='predicted heat load', linewidth=1.2)\n",
    "plt.xlabel('time (days)')\n",
    "plt.ylabel('heat load (×1e8)')\n",
    "plt.title('heat load — actual vs predicted')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#efficiency (y2)\n",
    "n2 = min(len(true_y2), len(pred_y2))\n",
    "x_days_y2 = np.arange(n2) / 96.0\n",
    "\n",
    "plt.figure(figsize=(11, 5))\n",
    "plt.plot(x_days_y2, true_y2[:n2], label='actual efficiency', linewidth=1.5)\n",
    "plt.plot(x_days_y2, pred_y2[:n2], label='predicted efficiency', linewidth=1.2)\n",
    "plt.xlabel('time (days)')\n",
    "plt.ylabel('efficiency')\n",
    "plt.title('efficiency — actual vs predicted')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38721fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import joblib\n",
    "\n",
    "#load the single global y scaler if not already in memory\n",
    "#Y_scaler_global = joblib.load(\"artifacts/Y_scaler_global.joblib\")\n",
    "\n",
    "#evaluate unscaled MAPE for each dataset (y1 and y2), supporting output_width > 1\n",
    "model.eval()\n",
    "mape_per_dataset = []         \n",
    "total_y1_mape_sum = 0.0      \n",
    "total_y2_mape_sum = 0.0        \n",
    "total_samples = 0              \n",
    "total_mape_sum = 0.0           \n",
    "\n",
    "for dataset_idx, (X_test, Y_test_scaled) in enumerate((ds[2], ds[5]) for ds in datasets_windowed):\n",
    "    X_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        Y_pred_scaled = model(X_tensor).cpu().numpy()\n",
    "\n",
    "    Y_pred_unscaled = Y_scaler_global.inverse_transform(Y_pred_scaled)\n",
    "    Y_true_unscaled = Y_scaler_global.inverse_transform(Y_test_scaled)\n",
    "\n",
    "    Y_all_true_flat = Y_true_unscaled.flatten()\n",
    "    Y_all_pred_flat = Y_pred_unscaled.flatten()\n",
    "    total_mape_sum += mean_absolute_percentage_error(Y_all_true_flat, Y_all_pred_flat) * 100 * len(Y_all_true_flat)\n",
    "\n",
    "    Y1_true = Y_true_unscaled[:, ::2]\n",
    "    Y2_true = Y_true_unscaled[:, 1::2]\n",
    "    Y1_pred = Y_pred_unscaled[:, ::2]\n",
    "    Y2_pred = Y_pred_unscaled[:, 1::2]\n",
    "\n",
    "    Y1_true_flat = Y1_true.flatten()\n",
    "    Y1_pred_flat = Y1_pred.flatten()\n",
    "    Y2_true_flat = Y2_true.flatten()\n",
    "    Y2_pred_flat = Y2_pred.flatten()\n",
    "\n",
    "    #compute MAPE for y1 and y2\n",
    "    mape_y1 = mean_absolute_percentage_error(Y1_true_flat, Y1_pred_flat) * 100\n",
    "    mape_y2 = mean_absolute_percentage_error(Y2_true_flat, Y2_pred_flat) * 100\n",
    "\n",
    "    mape_per_dataset.append((mape_y1, mape_y2))\n",
    "    print(f\"dataset {dataset_idx + 1}: y1 mape = {mape_y1:.4f}%, y2 mape = {mape_y2:.4f}%\")\n",
    "\n",
    "    total_y1_mape_sum += mape_y1 * len(Y1_true_flat)\n",
    "    total_y2_mape_sum += mape_y2 * len(Y2_true_flat)\n",
    "    total_samples += len(Y1_true_flat)\n",
    "\n",
    "#global average mape (weighted by sample count)\n",
    "avg_y1_mape = total_y1_mape_sum / total_samples\n",
    "avg_y2_mape = total_y2_mape_sum / total_samples\n",
    "\n",
    "print(\"\\nWeighted average mape across all datasets:\")\n",
    "print(f\"y1: {avg_y1_mape:.4f}%\")\n",
    "print(f\"y2: {avg_y2_mape:.4f}%\")\n",
    "\n",
    "#overall combined mape across both outputs and all time steps\n",
    "overall_mape = total_mape_sum / (total_samples * 2)\n",
    "print(f\"\\nOverall combined mape across all outputs and time steps:\\n{overall_mape:.4f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db089e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAPE CODE IF OUTPUT WIDTH is > 1\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "#load the single global y scaler if not already presen#\n",
    "# Y_scaler_global = joblib.load(\"artifacts/Y_scaler_global.joblib\")\n",
    "\n",
    "#evaluate unscaled MAPE for each dataset (y1 and y2), supporting output_width > 1\n",
    "model.eval()\n",
    "mape_per_dataset = []   \n",
    "total_y1_mape_sum = 0.0\n",
    "total_y2_mape_sum = 0.0\n",
    "total_samples = 0       \n",
    "total_mape_sum = 0.0    \n",
    "\n",
    "for dataset_idx, ds in enumerate(datasets_windowed, start=1):\n",
    "    X_test, Y_test_scaled = ds[2], ds[5]\n",
    "\n",
    "    X_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        Y_pred_scaled = model(X_tensor).cpu().numpy()\n",
    "\n",
    "    n_samples = Y_pred_scaled.shape[0]\n",
    "    output_width = Y_pred_scaled.shape[1] // 2\n",
    "\n",
    "    Y_pred_steps = Y_pred_scaled.reshape(n_samples * output_width, 2)\n",
    "    Y_true_steps = Y_test_scaled.reshape(n_samples * output_width, 2)\n",
    "\n",
    "    Y_pred_unscaled_steps = Y_scaler_global.inverse_transform(Y_pred_steps)\n",
    "    Y_true_unscaled_steps = Y_scaler_global.inverse_transform(Y_true_steps)\n",
    "\n",
    "    Y_pred_unscaled = Y_pred_unscaled_steps.reshape(n_samples, output_width * 2)\n",
    "    Y_true_unscaled = Y_true_unscaled_steps.reshape(n_samples, output_width * 2)\n",
    "\n",
    "    Y_all_true_flat = Y_true_unscaled.flatten()\n",
    "    Y_all_pred_flat = Y_pred_unscaled.flatten()\n",
    "    total_mape_sum += mean_absolute_percentage_error(Y_all_true_flat, Y_all_pred_flat) * 100 * len(Y_all_true_flat)\n",
    "\n",
    "    Y1_true = Y_true_unscaled[:, ::2]\n",
    "    Y2_true = Y_true_unscaled[:, 1::2]\n",
    "    Y1_pred = Y_pred_unscaled[:, ::2]\n",
    "    Y2_pred = Y_pred_unscaled[:, 1::2]\n",
    "\n",
    "    Y1_true_flat = Y1_true.flatten()\n",
    "    Y1_pred_flat = Y1_pred.flatten()\n",
    "    Y2_true_flat = Y2_true.flatten()\n",
    "    Y2_pred_flat = Y2_pred.flatten()\n",
    "\n",
    "    mape_y1 = mean_absolute_percentage_error(Y1_true_flat, Y1_pred_flat) * 100\n",
    "    mape_y2 = mean_absolute_percentage_error(Y2_true_flat, Y2_pred_flat) * 100\n",
    "\n",
    "    mape_per_dataset.append((mape_y1, mape_y2))\n",
    "    print(f\"dataset {dataset_idx}: y1 mape = {mape_y1:.4f}%, y2 mape = {mape_y2:.4f}%\")\n",
    "\n",
    "    #weighted accumulation for per-target averages\n",
    "    total_y1_mape_sum += mape_y1 * len(Y1_true_flat)\n",
    "    total_y2_mape_sum += mape_y2 * len(Y2_true_flat)\n",
    "    total_samples += len(Y1_true_flat) \n",
    "\n",
    "#final aggregated results\n",
    "avg_y1_mape = total_y1_mape_sum / total_samples\n",
    "avg_y2_mape = total_y2_mape_sum / total_samples\n",
    "overall_mape = total_mape_sum / (total_samples * 2) \n",
    "\n",
    "print(\"\\nWeighted average mape across all datasets:\")\n",
    "print(f\"y1: {avg_y1_mape:.4f}%\")\n",
    "print(f\"y2: {avg_y2_mape:.4f}%\")\n",
    "print(f\"\\nOverall combined mape across all outputs and time steps:\\n{overall_mape:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b4c0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAPE per time-step (horizon) for output_width = 8\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import joblib, numpy as np, torch\n",
    "\n",
    "\n",
    "#Y_scaler_global = joblib.load(\"artifacts/Y_scaler_global.joblib\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "overall_y1_h_sum = None   \n",
    "overall_y2_h_sum = None\n",
    "overall_both_h_sum = None\n",
    "overall_h_counts  = None  #\n",
    "\n",
    "#print flag for per-dataset details\n",
    "PRINT_PER_DATASET = False\n",
    "\n",
    "#iterate through test sets in datasets_windowed\n",
    "for dataset_idx, (X_test, Y_test_scaled) in enumerate([(ds[2], ds[5]) for ds in datasets_windowed]):\n",
    "    if X_test is None or len(X_test) == 0:\n",
    "        continue\n",
    "\n",
    "    X_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        Y_pred_scaled = model(X_tensor).cpu().numpy()\n",
    "\n",
    "    n_samples = Y_pred_scaled.shape[0]\n",
    "    output_width = Y_pred_scaled.shape[1] // 2  # two outputs\n",
    "\n",
    "    Y_pred_steps = Y_pred_scaled.reshape(n_samples * output_width, 2)\n",
    "    Y_true_steps = Y_test_scaled.reshape(n_samples * output_width, 2)\n",
    "    Y_pred_unscaled_steps = Y_scaler_global.inverse_transform(Y_pred_steps)\n",
    "    Y_true_unscaled_steps = Y_scaler_global.inverse_transform(Y_true_steps)\n",
    "\n",
    "    Y_pred = Y_pred_unscaled_steps.reshape(n_samples, output_width, 2)\n",
    "    Y_true = Y_true_unscaled_steps.reshape(n_samples, output_width, 2)\n",
    "\n",
    "    Y1_pred, Y2_pred = Y_pred[:, :, 0], Y_pred[:, :, 1]\n",
    "    Y1_true, Y2_true = Y_true[:, :, 0], Y_true[:, :, 1]\n",
    "\n",
    "    if overall_y1_h_sum is None:\n",
    "        overall_y1_h_sum = np.zeros(output_width, dtype=float)\n",
    "        overall_y2_h_sum = np.zeros(output_width, dtype=float)\n",
    "        overall_both_h_sum = np.zeros(output_width, dtype=float)\n",
    "        overall_h_counts  = np.zeros(output_width, dtype=int)\n",
    "\n",
    "    if PRINT_PER_DATASET:\n",
    "        print(f\"\\n=== Dataset {dataset_idx + 1} MAPE by Horizon ===\")\n",
    "\n",
    "    for h in range(output_width):  # h = 0..7 → t+1..t+8\n",
    "        y1_t, y1_p = Y1_true[:, h].ravel(), Y1_pred[:, h].ravel()\n",
    "        y2_t, y2_p = Y2_true[:, h].ravel(), Y2_pred[:, h].ravel()\n",
    "        n_h = len(y1_t)\n",
    "\n",
    "        mape_y1_h = mean_absolute_percentage_error(y1_t, y1_p) * 100.0\n",
    "        mape_y2_h = mean_absolute_percentage_error(y2_t, y2_p) * 100.0\n",
    "        mape_both_h = mean_absolute_percentage_error(\n",
    "            np.concatenate([y1_t, y2_t]),\n",
    "            np.concatenate([y1_p, y2_p])\n",
    "        ) * 100.0\n",
    "\n",
    "\n",
    "        overall_y1_h_sum[h]  += mape_y1_h * n_h\n",
    "        overall_y2_h_sum[h]  += mape_y2_h * n_h\n",
    "        overall_both_h_sum[h] += mape_both_h * (2 * n_h)\n",
    "        overall_h_counts[h]  += n_h\n",
    "\n",
    "        if PRINT_PER_DATASET:\n",
    "            print(f\"H{h + 1}: Y1 {mape_y1_h:.2f}% | Y2 {mape_y2_h:.2f}% | Both {mape_both_h:.2f}% (n={n_h})\")\n",
    "\n",
    "#overall (weighted) MAPE per horizon across all dataset\n",
    "overall_y1_h = overall_y1_h_sum / np.maximum(overall_h_counts, 1)\n",
    "overall_y2_h = overall_y2_h_sum / np.maximum(overall_h_counts, 1)\n",
    "overall_both_h = overall_both_h_sum / np.maximum(2 * overall_h_counts, 1)\n",
    "\n",
    "print(\"\\n=== Overall MAPE by Horizon (t+1..t+8) Across All Datasets ===\")\n",
    "for h in range(output_width):\n",
    "    print(f\"H{h + 1}:  Y1 {overall_y1_h[h]:.2f}%   Y2 {overall_y2_h[h]:.2f}%   Both {overall_both_h[h]:.2f}%\")\n",
    "\n",
    "#report averages across horizons\n",
    "print(\"\\n=== Averages Across Horizons (Weighted) ===\")\n",
    "print(f\"Y1 Avg:  {overall_y1_h.mean():.2f}%\")\n",
    "print(f\"Y2 Avg:  {overall_y2_h.mean():.2f}%\")\n",
    "print(f\"Both Avg: {overall_both_h.mean():.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
